import requests
import xml.etree.ElementTree as ET
import json
import re
import time
import os
import sys

# Garantir que o script consegue importar m√≥dulos vizinhos se corrido da raiz
script_dir = os.path.dirname(os.path.abspath(__file__))
if script_dir not in sys.path:
    sys.path.append(script_dir)

from typing import List, Dict
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

def fetch_rss_feed(url: str) -> str:
    """
    Faz fetch do conte√∫do XML do RSS feed
    """
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"Erro ao fazer fetch do RSS feed: {e}")
        return None

def setup_driver():
    """
    Configura e retorna o driver do Chrome usando webdriver-manager para baixar a vers√£o correta
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-images")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("--silent")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
 
    
    # Usar webdriver-manager para baixar a vers√£o correta do Chrome WebDriver
    chrome_driver_path = ChromeDriverManager().install()
    print(f"Usando Chrome WebDriver: {chrome_driver_path}")
    
    service = Service(chrome_driver_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def fetch_procedure_details(driver, url: str) -> Dict[str, str]:
    """
    Extrai detalhes de um procedimento espec√≠fico a partir da URL usando um driver j√° existente
    """
    if not driver:
        print("Erro: Driver n√£o fornecido")
        return None
        
    try:
        print(f"Acessando: {url}")
        driver.get(url)
        
        # Aguardar carregamento da p√°gina
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Aguardar um pouco para garantir que o JavaScript carregou
        time.sleep(3)
        
        # Obter o HTML renderizado
        page_source = driver.page_source
        
        # Usar BeautifulSoup para parsear o HTML
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # Procurar pelo texto espec√≠fico
        possible_texts = [
            "1 - IDENTIFICA√á√ÉO E CONTACTOS DA ENTIDADE ADJUDICANTE",
            "IDENTIFICA√á√ÉO E CONTACTOS DA ENTIDADE ADJUDICANTE",
            "IDENTIFICA√á√ÉO"
        ]
        
        target_element = None
        for text in possible_texts:
            target_element = soup.find(string=re.compile(text, re.IGNORECASE))
            if target_element:
                break
        
        if target_element:
            parent_div = target_element.find_parent('div')
            if parent_div:
                details_text = parent_div.get_text(separator='\n', strip=True)
                
                extracted_info = {}
                patterns = {
                    'entidade': r'Designa√ß√£o da entidade adjudicante:\s*(.+?)(?:\n|$)',
                    'nipc': r'NIPC:\s*(\d+)',
                    'distrito': r'Distrito:\s*(.+?)(?:\n|$)',
                    'concelho': r'Concelho:\s*(.+?)(?:\n|$)',
                    'freguesia': r'Freguesia:\s*(.+?)(?:\n|$)',
                    'site': r'Endere√ßo da Entidade \(URL\):\s*(.+?)(?:\n|$)',
                    'email': r'Endere√ßo Eletr√≥nico:\s*(.+?)(?:\n|$)',
                    'designacao_contrato': r'Designa√ß√£o do contrato:\s*(.+?)(?:\n|$)',
                    'descricao': r'Descri√ß√£o:\s*(.+?)(?:\n|$)',
                    'preco_base': r'Pre√ßo base s/IVA:\s*(.+?)(?:\n|$)',
                    'prazo_execucao': r'Prazo de execu√ß√£o do contrato:\s*(.+?)(?:\n|$)',
                    'prazo_apresentacao_propostas': r'Prazo para apresenta√ß√£o das propostas:\s*(.+?)(?:\n|$)',
                    'fundos_eu': r'T√™m fundos EU\?\s*(.+?)(?:\n|$)',
                    'plataforma_eletronica': r'Plataforma eletr√≥nica utilizada pela entidade adjudicante:\s*(.+?)(?:\n|$)',
                    'url_procedimento': r'URL para Apresenta√ß√£o:\s*(.+?)(?:\n|$)',
                    'autor_nome': r'28 - IDENTIFICA√á√ÉO DO\(S\) AUTOR\(ES\) DE AN√öNCIO\nNome:\s*(.+?)(?:\n|$)',
                    'autor_cargo': r'Cargo:\s*(.+?)(?:\n|$)'
                }
                
                for field, pattern in patterns.items():
                    match = re.search(pattern, details_text, re.MULTILINE | re.DOTALL)
                    if match:
                        value = match.group(1).strip()
                        value = re.sub(r'\s+', ' ', value)
                        extracted_info[field] = value
                    else:
                        extracted_info[field] = None
                
                return {
                    'detalhes_completos': details_text,
                    **extracted_info
                }
        
        return None
        
    except Exception as e:
        print(f"Erro ao extrair detalhes: {e}")
        return None


def extract_procedure_info(title: str, description: str) -> Dict[str, str]:
    """
    Extrai n√∫mero do procedimento e entidade do t√≠tulo/descri√ß√£o
    """
    # Padr√£o para extrair n√∫mero do procedimento
    numero_match = re.search(r'n\.¬∫\s*(\d+)/\d+', title)
    numero_procedimento = numero_match.group(1) if numero_match else "N/A"
    
    # Extrair entidade (geralmente est√° no t√≠tulo ap√≥s o n√∫mero)
    entidade = title.strip()
    
    return {
        "numero_procedimento": numero_procedimento,
        "entidade": entidade
    }

def parse_rss_to_json(xml_content: str) -> List[Dict[str, str]]:
    """
    Faz parse do XML do RSS feed e extrai informa√ß√µes dos procedimentos
    """
    try:
        root = ET.fromstring(xml_content)
        
        # Namespace para RSS
        ns = {'rss': 'http://purl.org/rss/1.0/'}
        
        extracted_data = []
        
        # Encontrar todos os items
        for item in root.findall('.//item'):
            title_elem = item.find('title')
            description_elem = item.find('description')
            link_elem = item.find('link')
            
            if title_elem is not None and link_elem is not None:
                title = title_elem.text or ""
                description = description_elem.text if description_elem is not None else ""
                link = link_elem.text or ""
                
                # Limpar CDATA se presente
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                
                # Extrair informa√ß√µes do procedimento
                procedure_info = extract_procedure_info(title, description)
                
                item_data = {
                    "numero_procedimento": procedure_info["numero_procedimento"],
                    "entidade": procedure_info["entidade"],
                    "link": link
                }
                
                extracted_data.append(item_data)
        
        return extracted_data
        
    except ET.ParseError as e:
        print(f"Erro ao fazer parse do XML: {e}")
        return []

def save_to_json(data: List[Dict[str, str]], filename: str = "procedimentos_dre.json"):
    """
    Salva os dados extra√≠dos em formato JSON em todas as localiza√ß√µes encontradas
    """
    targets = []
    # Root RSS
    if os.path.exists('RSS') or os.path.exists('package.json'): targets.append('RSS')
    elif os.path.exists('../RSS') or os.path.exists('../package.json'): targets.append('../RSS')
    
    # Public RSS
    if os.path.exists('public/RSS'): targets.append('public/RSS')
    elif os.path.exists('../public/RSS'): targets.append('../public/RSS')
    
    if not targets: targets = ['RSS']
    
    for rss_dir in targets:
        try:
            os.makedirs(rss_dir, exist_ok=True)
            filepath = os.path.join(rss_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ Dados salvos em {filepath}")
        except Exception as e:
            print(f"‚ùå Erro ao salvar em {rss_dir}: {e}")

def save_to_json_with_date(data: List[Dict[str, str]]):
    """
    Salva os dados extra√≠dos em formato JSON na pasta data/ com nome baseado na data atual
    """
    try:
        from datetime import datetime
        # Obter data atual no formato DD-MM-YYYY
        current_date = datetime.now().strftime('%d-%m-%Y')
        filename = f"{current_date}.json"
        
        targets = []
        # Root data
        if os.path.exists('data') or os.path.exists('package.json'): targets.append('data')
        elif os.path.exists('../data') or os.path.exists('../package.json'): targets.append('../data')
        
        # Public data
        if os.path.exists('public/data'): targets.append('public/data')
        elif os.path.exists('../public/data'): targets.append('../public/data')
        
        if not targets: targets = ['data']
        
        last_path = None
        for data_dir in targets:
            try:
                os.makedirs(data_dir, exist_ok=True)
                filepath = os.path.join(data_dir, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"‚úÖ Dados salvos com data em {filepath}")
                last_path = filepath
            except Exception as e:
                print(f"‚ùå Erro ao salvar em {data_dir}: {e}")
                
        return last_path
    except Exception as e:
        print(f"‚ùå Erro ao processar save_to_json_with_date: {e}")
        return None

def main():
    """
    Fun√ß√£o principal que executa todo o processo
    """
    rss_url = "https://files.diariodarepublica.pt/rss/serie2&parte=l-html.xml"
    
    print("Fazendo fetch do RSS feed do Di√°rio da Rep√∫blica...")
    xml_content = fetch_rss_feed(rss_url)
    
    if xml_content is None:
        print("N√£o foi poss√≠vel obter o conte√∫do do RSS feed")
        return
    
    print("Extraindo informa√ß√µes dos procedimentos...")
    extracted_data = parse_rss_to_json(xml_content)
    
    if not extracted_data:
        print("Nenhum dado foi extra√≠do")
        return
    
    print(f"Extra√≠dos {len(extracted_data)} procedimentos")
    
    # Salvar dados b√°sicos em JSON
    save_to_json(extracted_data, "procedimentos_basicos.json")
    
    # Carregar base de dados existente para evitar re-scraping
    existing_data = {}
    try:
        possible_completo_paths = ['RSS/procedimentos_completos.json', 'public/RSS/procedimentos_completos.json']
        for p in possible_completo_paths:
            if os.path.exists(p):
                with open(p, 'r', encoding='utf-8') as f:
                    data_list = json.load(f)
                    for d in data_list:
                        if 'link' in d: existing_data[d['link']] = d
                break
    except: pass

    # Extrair detalhes de cada procedimento
    print(f"\nExtraindo detalhes de {len(extracted_data)} procedimentos...")
    procedimentos_completos = []
    
    driver = setup_driver()
    try:
        for i, item in enumerate(extracted_data):
            link = item.get('link')
            print(f"\n[{i+1}/{len(extracted_data)}] {item['numero_procedimento']}")
            
            # Se j√° temos os detalhes, saltar
            if link in existing_data and existing_data[link].get('detalhes_completos'):
                print(f"  ‚ö° J√° existe na base de dados, ignorando fetch")
                procedimentos_completos.append(existing_data[link])
                continue

            # Extrair detalhes do procedimento
            details = fetch_procedure_details(driver, link)
            
            if details:
                item_completo = {**item, **details}
                procedimentos_completos.append(item_completo)
                print(f"  ‚úì Detalhes extra√≠dos")
            else:
                procedimentos_completos.append(item)
                print(f"  ‚úó Falha na extra√ß√£o de detalhes")
    finally:
        if driver: driver.quit()
    
    # Salvar dados completos em JSON
    save_to_json(procedimentos_completos, "procedimentos_completos.json")
    
    # Salvar dados completos em JSON com data na pasta data/
    print("\nüìÖ Salvando dados com data atual...")
    data_file_path = save_to_json_with_date(procedimentos_completos)
    
    # Atualizar arquivo ativos.json
    print("\nüîÑ Atualizando arquivo ativos.json...")
    try:
        from gerir_ativos import update_ativos_from_date_file, merge_with_existing_ativos, save_ativos
        from notify_new_items import notify_new_items
        
        if data_file_path:
            # Obter procedimentos ativos do arquivo de data
            procedimentos_ativos = update_ativos_from_date_file(data_file_path)
            
            # --- NOTIFICA√á√ÉO ---
            # Notificar ANTES de fazer o merge definitivo (para saber o que √© realmente novo)
            print("üì¨ Verificando notifica√ß√µes para novos itens...")
            notify_new_items(procedimentos_ativos)
            # -------------------
            
            # Combinar com procedimentos ativos existentes
            ativos_finais = merge_with_existing_ativos(procedimentos_ativos)
            
            # Salvar arquivo ativos.json
            ativos_file_path = save_ativos(ativos_finais)
            
            if ativos_file_path:
                print(f"‚úÖ Arquivo ativos.json atualizado com sucesso!")
                print(f"üìä Total de procedimentos ativos: {len(ativos_finais)}")
            else:
                print("‚ùå Erro ao salvar arquivo ativos.json")
        else:
            print("‚ùå N√£o foi poss√≠vel obter caminho do arquivo de data")
            
    except Exception as e:
        print(f"‚ùå Erro ao atualizar ativos.json: {e}")
    
    # Gerar automaticamente o feed RSS
    print("\nüîÑ Gerando feed RSS automaticamente...")
    try:
        import subprocess
        import sys
        
        print("Executando conversor JSON para RSS...")
        result = subprocess.run([sys.executable, "json_to_rss_converter.py"], 
                              capture_output=True, text=True, check=True)
        
        print("‚úÖ Feed RSS gerado com sucesso!")
        print("üìÑ Arquivo criado: ../public/RSS/feed_rss_procedimentos.xml")
        
        # Mostrar estat√≠sticas do feed RSS
        if "Total de procedimentos processados:" in result.stdout:
            print("\nüìä Estat√≠sticas do Feed RSS:")
            for line in result.stdout.split('\n'):
                if "Estat√≠sticas:" in line or "procedimentos" in line:
                    print(f"  {line}")
                    
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erro ao gerar feed RSS: {e}")
        print(f"Erro: {e.stderr}")
    except Exception as e:
        print(f"‚ùå Erro inesperado ao gerar feed RSS: {e}")
    
    # --- GERA√á√ÉO DE RSS FILTRADO (SEEDS) ---
    print("\nüì° Gerando RSS personalizado (Seeds)...")
    try:
        from generate_filtered_rss import generate_filtered_rss
        generate_filtered_rss()
    except Exception as e:
        print(f"‚ùå Erro ao gerar RSS filtrado: {e}")
    # -------------------------------------
    
    print(f"\nüéâ Processo completo finalizado!")
    print(f"Procedimentos processados: {len(procedimentos_completos)}")
    print(f"üìÅ Arquivos gerados:")
    print(f"  - public/RSS/procedimentos_basicos.json (dados do RSS)")
    print(f"  - public/RSS/procedimentos_completos.json (dados + detalhes)")
    if data_file_path:
        print(f"  - {data_file_path} (dados completos com data)")
    print(f"  - public/data/ativos.json (procedimentos ativos)")
    print(f"  - public/RSS/feed_rss_procedimentos.xml (feed RSS completo)")
    print(f"  - public/RSS/feed_filtros_seeds.xml (feed RSS filtrado por SEEDS)")

if __name__ == "__main__":
    main() 